{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectX with Minimax-DQN\n",
    "In this notebook I show a simple extension of [DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) to 2 player alternating-turn games known as Minimax-DQN. [Minimax-DQN](https://arxiv.org/abs/1901.00137) is based on [Minimax Q learning](https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf) a paper from 1994. Minimax-DQN is the modern neural network equivalent to the tabular Q learning version.\n",
    "\n",
    "## DQN\n",
    "Vanilla DQN isn't meant for the Multi-Agent Reinforcement Learning (MARL) setting, but we can use DQN here to learn against a fixed opponent and consider it as part of the environment. For example in [this notebook](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning/notebook), off of which my notebook was forked, DQN is used to learn the game by playing against a random opponent. Note that a random opponent is considered \"fixed\" in the sense that its policy does not change over time.\n",
    "\n",
    "## IQL\n",
    "To learn in self play, one *can* simply pit 2 DQN agents against each other in the environment but this violates the fundamental assumption of Q learning that the environment is fixed. This is referred to as Independent Q learning (IQL) in this [paper](https://ora.ox.ac.uk/objects/uuid:2b650b3b-2fce-4875-b4df-70f4a4d64c8a/download_file?file_format=pdf&safe_filename=main.pdf&type_of_work=Conference+item) which discussed a way of making this technique work better in practice.\n",
    "\n",
    "## Minimax DQN\n",
    "It is however possible to do a more principled extension of DQN. In DQN we have the bellman equation \n",
    "\n",
    "$Q(s,p)=r+\\gamma max_a Q(s',a)$\n",
    "\n",
    "with $s$ the current state, $s'$ the next state, $p$ the action that lead from $s$ to $s'$, $r$ the immediate reward of going from state $s$ to $s'$ due to action $p$ and $\\gamma$ the discount factor of DQN.\n",
    "\n",
    "For a 1v1 alternating-turn game we can formalise our problem as a zero-sum game, where 1 is a certain win, -1 is a certain loss and the bellman equation becomes\n",
    "\n",
    "$Q(s,p,player\\_0)=r-\\gamma max_a Q(s',a,player\\_1)$\n",
    "\n",
    "Note that this equation is very much akin to what is done in [Negamax](https://en.wikipedia.org/wiki/Negamax#Negamax_base_algorithm).\n",
    "\n",
    "## \"Nash-DQN\"\n",
    "For your curiosity, Minimax-DQN can be generalised to 1v1 simultaneous-turn games by having the network output a matrix of Q values of size \\[N_actions,N_Actions\\] representing the value of each possible pairs of actions for both players. This can then be viewed as the payoff matrix of the game which can be solved with algorithms [such as this one](http://code.activestate.com/recipes/496825-game-theory-payoff-matrix-solver/). This again allows to transform the game back into a single agent form. More details in [this article](https://github.com/pb4git/Nash-DQN-CSB-Article) where me and a friend used this technique on another game.\n",
    "\n",
    "## Further improvements\n",
    "* The DQN network learned in this manner could be wrapped in a Negamax search, which would, in my experience, yield massive elo gains. The deep Q network can be used as the evaluation function required in a Negamax agent.\n",
    "* DQN can be improved with [Double DQN (DDQN)](https://arxiv.org/abs/1509.06461), [Prioritised Experience Replay](https://arxiv.org/abs/1511.05952), [Duelling DQN](https://arxiv.org/abs/1511.06581), [Noisy DQN](https://arxiv.org/abs/1706.10295) and the other improvements detailed in [the Rainbow paper](https://arxiv.org/abs/1710.02298).\n",
    "\n",
    "This agent currently has a leaderboard score of ~1050 playing greedily on the Q values. The current \\#1 player [is also using this technique](https://www.kaggle.com/c/connectx/discussion/129145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.10\n",
      "  Using cached tensorflow-2.10.0-cp37-cp37m-macosx_10_14_x86_64.whl (241.1 MB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (22.10.26)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: setuptools in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (63.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (4.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (1.21.5)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp37-cp37m-macosx_10_9_x86_64.whl (979 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (1.42.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (1.1.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (1.16.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (1.1.2)\n",
      "Requirement already satisfied: packaging in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (21.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (0.2.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorflow==2.10) (0.27.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from packaging->tensorflow==2.10) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.11.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.26.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.8.0)\n",
      "Installing collected packages: protobuf, astunparse, absl-py, tensorboard, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.0.2\n",
      "    Uninstalling tensorboard-2.0.2:\n",
      "      Successfully uninstalled tensorboard-2.0.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.0.0\n",
      "    Uninstalling tensorflow-2.0.0:\n",
      "      Successfully uninstalled tensorflow-2.0.0\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 protobuf-3.19.6 tensorboard-2.10.1 tensorflow-2.10.0\n",
      "Requirement already satisfied: tqdm in /Users/school/opt/anaconda3/envs/latest/lib/python3.7/site-packages (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.10\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "Train = True\n",
    "gamma = 0.99\n",
    "copy_step = 250\n",
    "hidden_units = [100, 200, 200, 100]\n",
    "max_experiences = 1000000\n",
    "min_experiences = 100000\n",
    "Steps_Till_Backprop = 64\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "epsilon = 0.99\n",
    "decay = 0.9999\n",
    "min_epsilon = 0.1\n",
    "episodes = 2000 #Set this to longer\n",
    "precision = 5\n",
    "Discard_Q_Value = -1e7\n",
    "Metric_Titles = ['Max_Q','Avg_Q','Min_Q']\n",
    "N_Downsampling_Episodes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI gym environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = make('connectx', debug=False)\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.env.state\n",
    "\n",
    "    def game_over(self):\n",
    "        return self.env.done\n",
    "\n",
    "    def current_player(self):\n",
    "        active = -1\n",
    "        if self.env.state[0].status == \"ACTIVE\":\n",
    "            active=0\n",
    "        if self.env.state[1].status == \"ACTIVE\":\n",
    "            active=1\n",
    "        return active\n",
    "\n",
    "    def get_configuration(self):\n",
    "        return self.env.configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
    "        self.hidden_layers = []\n",
    "        for i in hidden_units:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(i, activation='relu', kernel_initializer='he_normal'))\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions, activation='tanh', kernel_initializer='RandomNormal')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the tanh output layer which restricts output to \\[-1,1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.keras.optimizers.Nadam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = DeepModel(num_states, hidden_units, num_actions)\n",
    "        self.experience = {'inputs': [], 'a': [], 'r': [], 'inputs2': [], 'done': []} # The buffer\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
    "\n",
    "    #@tf.function\n",
    "    def train(self, TargetNet):\n",
    "        # Only start the training process when we have enough experiences in the buffer\n",
    "        if len(self.experience['inputs']) < self.min_experiences:\n",
    "            return 0\n",
    "\n",
    "        # Randomly select n experience in the buffer, n is batch-size\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['inputs']), size=self.batch_size)\n",
    "        states = np.asarray([self.experience['inputs'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "\n",
    "        # Prepare labels for training process\n",
    "        states_next = np.asarray([self.experience['inputs2'][i] for i in ids])\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        \n",
    "        # Find the value of the next states by computing the max over valid actions in these next states\n",
    "        Move_Validity = states_next[:,:self.num_actions]==0\n",
    "        Next_Q_Values = TargetNet.predict(states_next)\n",
    "        Next_Q_Values = np.where(Move_Validity,Next_Q_Values,Discard_Q_Value)\n",
    "        value_next = -np.max(Next_Q_Values,axis=1)\n",
    "        \n",
    "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            selected_action_values = tf.math.reduce_sum(self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
    "            loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Get an action by using epsilon-greedy\n",
    "    def get_action(self, state, epsilon):\n",
    "        prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n",
    "        if np.random.random() < epsilon:\n",
    "            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0])), prediction\n",
    "        else:\n",
    "            for i in range(self.num_actions):\n",
    "                if state.board[i] != 0:\n",
    "                    prediction[i] = Discard_Q_Value\n",
    "            return int(np.argmax(prediction)) , prediction\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['inputs']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        ref_model = tf.keras.Sequential()\n",
    "\n",
    "        ref_model.add(self.model.input_layer)\n",
    "        for layer in self.model.hidden_layers:\n",
    "            ref_model.add(layer)\n",
    "        ref_model.add(self.model.output_layer)\n",
    "\n",
    "        ref_model.load_weights(path)\n",
    "    \n",
    "    # Each state is represented as 1s for the current player's pieces, -1s for the opponent's pieces and 0s\n",
    "    def preprocess(self, state):\n",
    "        return np.array([1 if val==state.mark else 0 if val==0 else -1 for val in state.board])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that in the DQN::preprocess() method we represent the state for the current player. The current player (given by state.mark==1/2) will see his pieces as 1s and the opponent's pieces as -1s. Once he plays, the next player will see the state \"from his point of view\" where all *his* pieces are 1s. That way the Neural network does not make a distinction between playing player 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will play 1 ConnectX game and learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter):\n",
    "    turns = 0\n",
    "    env.reset()\n",
    "    Metric_Buffer={key:[] for key in Metric_Titles}\n",
    "    while not env.game_over():\n",
    "        active = env.current_player()\n",
    "\n",
    "        # Using epsilon-greedy to get an action\n",
    "        observations = env.get_state()[active].observation\n",
    "        try:\n",
    "            observations.board\n",
    "        except:\n",
    "            other_observations = env.get_state()[1-active].observation\n",
    "            observations.board = other_observations.board\n",
    "        action, Q_Values = TrainNet.get_action(observations, epsilon)\n",
    "        Q_Values = [val for val in Q_Values if val!=Discard_Q_Value]\n",
    "        Metric_Buffer['Avg_Q'].append(np.mean(Q_Values))\n",
    "        Metric_Buffer['Max_Q'].append(np.max(Q_Values))\n",
    "        Metric_Buffer['Min_Q'].append(np.min(Q_Values))\n",
    "\n",
    "        # Caching the information of current state\n",
    "        prev_observations = observations\n",
    "\n",
    "        # Take action\n",
    "        env.step([action if i==active else None for i in [0,1]])\n",
    "\n",
    "        reward=env.get_state()[active].reward\n",
    "\n",
    "        #Convert environment's [0,0.5,1] reward scheme to [-1,1]\n",
    "        if env.game_over():\n",
    "            if reward == 1: # Won\n",
    "                reward = 1\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -1\n",
    "            else: # Draw\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        next_active = 1 if active==0 else 0\n",
    "        # Adding experience into buffer\n",
    "        observations = env.get_state()[next_active].observation\n",
    "        try:\n",
    "            observations.board\n",
    "        except:\n",
    "            other_observations = env.get_state()[1-next_active].observation\n",
    "            observations.board = other_observations.board\n",
    "        exp = {'inputs': TrainNet.preprocess(prev_observations), 'a': action, 'r': reward, 'inputs2': TrainNet.preprocess(observations), 'done': env.game_over()}\n",
    "        TrainNet.add_experience(exp)\n",
    "\n",
    "        turns += 1\n",
    "        total_turns = Global_Step_Counter+turns\n",
    "        # Train the training model by using experiences in buffer and the target model\n",
    "        if total_turns%Steps_Till_Backprop==0:\n",
    "            TrainNet.train(TargetNet)\n",
    "        if total_turns%copy_step==0:\n",
    "            # Update the weights of the target model when reaching enough \"copy step\"\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    results={key:[] for key in Metric_Titles}\n",
    "    for metric_name in Metric_Titles:\n",
    "        results[metric_name]=np.mean(Metric_Buffer[metric_name])\n",
    "    return results, turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training loop. Note here that we track some metrics (min/max/mean Q values). Although it may be hard to tell from these metrics what constitutes a good agent, tracking them can help you ascertain convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n",
      "\n",
      "0it [01:29, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DeepModel.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa7c7cc03d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DeepModel.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fa7c7cc03d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 20/2000 [00:02<02:58, 11.12it/s, Steps=400, Updates=3200.0]\n",
      "\n",
      " 13%|█▎        | 260/2000 [00:26<02:23, 12.15it/s, Steps=5400, Updates=43200.0]=24]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 962/2000 [01:29<01:30, 11.53it/s, Steps=19400, Updates=1.55e+5]14]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 1082/2000 [01:40<01:18, 11.65it/s, Steps=21800, Updates=1.74e+5]6]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 1293/2000 [01:58<01:01, 11.50it/s, Steps=26000, Updates=2.08e+5]7]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1315/2000 [02:00<00:55, 12.32it/s, Steps=26400, Updates=2.11e+5]] \u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 1614/2000 [02:26<00:30, 12.51it/s, Steps=32000, Updates=2.56e+5] \u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2000/2000 [03:00<00:00, 11.11it/s, Steps=32000, Updates=2.56e+5]9]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "env = ConnectX()\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "Metrics = {key:[] for key in Metric_Titles} #Here we will store metrics for plotting after training\n",
    "Metrics_Buffer = {key:[] for key in Metric_Titles} #Downsampling buffer\n",
    "\n",
    "# Initialize models\n",
    "TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "\n",
    "if Train:\n",
    "    Global_Step_Counter=0\n",
    "    pbar = tqdm(range(episodes))\n",
    "    pbar2 = tqdm()\n",
    "    for n in pbar:\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        results, steps = play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter)\n",
    "        Global_Step_Counter += steps\n",
    "        for metric_name in Metric_Titles:\n",
    "            Metrics_Buffer[metric_name].append(results[metric_name])\n",
    "\n",
    "        if Global_Step_Counter%N_Downsampling_Episodes==0:\n",
    "            for metric_name in Metric_Titles: #Downsample our metrics from the buffer\n",
    "                Metrics[metric_name].append(np.mean(Metrics_Buffer[metric_name]))\n",
    "                Metrics_Buffer[metric_name].clear()\n",
    "            pbar.set_postfix({\n",
    "                'Steps': Global_Step_Counter,\n",
    "                'Updates': Global_Step_Counter*batch_size/Steps_Till_Backprop\n",
    "            })\n",
    "\n",
    "            pbar2.set_postfix({\n",
    "                'max_Q': Metrics['Max_Q'][-1],\n",
    "                'avg_Q': Metrics['Avg_Q'][-1],\n",
    "                'min_Q': Metrics['Min_Q'][-1],\n",
    "                'epsilon': epsilon,\n",
    "                'turns': steps\n",
    "            })\n",
    "\n",
    "    def Plot(data,title):\n",
    "        plt.figure()\n",
    "        plt.plot(data)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(title)\n",
    "        plt.savefig(title+'.png')\n",
    "        plt.close()\n",
    "\n",
    "    for metric_name in Metric_Titles:\n",
    "        Plot(Metrics[metric_name],metric_name)\n",
    "\n",
    "    TrainNet.save_weights('./weights.h5')\n",
    "else:\n",
    "    TrainNet.load_weights('./weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our agent into a submission.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = []\n",
    "\n",
    "# Get all hidden layers' weights\n",
    "for i in range(len(hidden_units)):\n",
    "    fc_layers.extend([\n",
    "        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n",
    "        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n",
    "    ])\n",
    "\n",
    "# Get output layer's weights\n",
    "fc_layers.extend([\n",
    "    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n",
    "    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n",
    "])\n",
    "\n",
    "# Convert all layers into usable form before integrating to final agent\n",
    "fc_layers = list(map(\n",
    "    lambda x: str(list(np.round(x, precision))) \\\n",
    "        .replace('array(', '').replace(')', '') \\\n",
    "        .replace(' ', '') \\\n",
    "        .replace('\\n', ''),\n",
    "    fc_layers\n",
    "))\n",
    "fc_layers = np.reshape(fc_layers, (-1, 2))\n",
    "\n",
    "# Create the agent\n",
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "\n",
    "'''\n",
    "\n",
    "# Write hidden layers\n",
    "for i, (w, b) in enumerate(fc_layers[:-1]):\n",
    "    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n",
    "    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n",
    "# Write output layer\n",
    "my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n",
    "my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n",
    "\n",
    "my_agent += '''\n",
    "    board = observation.board[:]\n",
    "    out = np.array([1 if val==observation.mark else 0 if val==0 else -1 for val in board],np.float32)\n",
    "'''\n",
    "\n",
    "# Calculate hidden layers\n",
    "for i in range(len(fc_layers[:-1])):\n",
    "    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n",
    "    my_agent += '    out = np.maximum(0,out)\\n' # Relu function\n",
    "# Calculate output layer\n",
    "my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n",
    "my_agent += '    out = np.tanh(out)\\n'\n",
    "\n",
    "my_agent += '''\n",
    "    for i in range(configuration.columns):\n",
    "        if observation.board[i] != 0:\n",
    "            out[i] = -1e7\n",
    "\n",
    "    return int(np.argmax(out))\n",
    "    '''\n",
    "\n",
    "with open('submission.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against the builtin Negamax opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error returning mean_reward: [[-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1]]\n",
      "My Agent vs. Negamax Agent: 0\n",
      "Error returning mean_reward: [[1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1], [1, -1]]\n",
      "Negamax Agent vs. My Agent: 0\n"
     ]
    }
   ],
   "source": [
    "Train = False\n",
    "from submission import my_agent\n",
    "\n",
    "def epsilon_greedify(agent, epsilon=0.05): #Greedify our agent so we don't play the same games over and over in strength evaluation\n",
    "    def greedified_agent(observation,configuration):\n",
    "        import random\n",
    "        if random.random()<epsilon:\n",
    "            return random.choice([i for i in range(num_actions) if observation.board[i]==0])\n",
    "        else:\n",
    "            return agent(observation,configuration)\n",
    "    return greedified_agent\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    try:\n",
    "        return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "    except:\n",
    "        print(\"Error returning mean_reward:\", rewards)\n",
    "        return 0\n",
    "\n",
    "my_agent = epsilon_greedify(my_agent)\n",
    "print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=20)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
