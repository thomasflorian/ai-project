{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectX with Minimax-DQN\n",
    "In this notebook I show a simple extension of [DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) to 2 player alternating-turn games known as Minimax-DQN. [Minimax-DQN](https://arxiv.org/abs/1901.00137) is based on [Minimax Q learning](https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf) a paper from 1994. Minimax-DQN is the modern neural network equivalent to the tabular Q learning version.\n",
    "\n",
    "## DQN\n",
    "Vanilla DQN isn't meant for the Multi-Agent Reinforcement Learning (MARL) setting, but we can use DQN here to learn against a fixed opponent and consider it as part of the environment. For example in [this notebook](https://www.kaggle.com/phunghieu/connectx-with-deep-q-learning/notebook), off of which my notebook was forked, DQN is used to learn the game by playing against a random opponent. Note that a random opponent is considered \"fixed\" in the sense that its policy does not change over time.\n",
    "\n",
    "## IQL\n",
    "To learn in self play, one *can* simply pit 2 DQN agents against each other in the environment but this violates the fundamental assumption of Q learning that the environment is fixed. This is referred to as Independent Q learning (IQL) in this [paper](https://ora.ox.ac.uk/objects/uuid:2b650b3b-2fce-4875-b4df-70f4a4d64c8a/download_file?file_format=pdf&safe_filename=main.pdf&type_of_work=Conference+item) which discussed a way of making this technique work better in practice.\n",
    "\n",
    "## Minimax DQN\n",
    "It is however possible to do a more principled extension of DQN. In DQN we have the bellman equation \n",
    "\n",
    "$Q(s,p)=r+\\gamma max_a Q(s',a)$\n",
    "\n",
    "with $s$ the current state, $s'$ the next state, $p$ the action that lead from $s$ to $s'$, $r$ the immediate reward of going from state $s$ to $s'$ due to action $p$ and $\\gamma$ the discount factor of DQN.\n",
    "\n",
    "For a 1v1 alternating-turn game we can formalise our problem as a zero-sum game, where 1 is a certain win, -1 is a certain loss and the bellman equation becomes\n",
    "\n",
    "$Q(s,p,player\\_0)=r-\\gamma max_a Q(s',a,player\\_1)$\n",
    "\n",
    "Note that this equation is very much akin to what is done in [Negamax](https://en.wikipedia.org/wiki/Negamax#Negamax_base_algorithm).\n",
    "\n",
    "## \"Nash-DQN\"\n",
    "For your curiosity, Minimax-DQN can be generalised to 1v1 simultaneous-turn games by having the network output a matrix of Q values of size \\[N_actions,N_Actions\\] representing the value of each possible pairs of actions for both players. This can then be viewed as the payoff matrix of the game which can be solved with algorithms [such as this one](http://code.activestate.com/recipes/496825-game-theory-payoff-matrix-solver/). This again allows to transform the game back into a single agent form. More details in [this article](https://github.com/pb4git/Nash-DQN-CSB-Article) where me and a friend used this technique on another game.\n",
    "\n",
    "## Further improvements\n",
    "* The DQN network learned in this manner could be wrapped in a Negamax search, which would, in my experience, yield massive elo gains. The deep Q network can be used as the evaluation function required in a Negamax agent.\n",
    "* DQN can be improved with [Double DQN (DDQN)](https://arxiv.org/abs/1509.06461), [Prioritised Experience Replay](https://arxiv.org/abs/1511.05952), [Duelling DQN](https://arxiv.org/abs/1511.06581), [Noisy DQN](https://arxiv.org/abs/1706.10295) and the other improvements detailed in [the Rainbow paper](https://arxiv.org/abs/1710.02298).\n",
    "\n",
    "This agent currently has a leaderboard score of ~1050 playing greedily on the Q values. The current \\#1 player [is also using this technique](https://www.kaggle.com/c/connectx/discussion/129145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "Train = True\n",
    "gamma = 0.99\n",
    "copy_step = 250\n",
    "hidden_units = [100, 200, 200, 100]\n",
    "max_experiences = 1000000\n",
    "min_experiences = 100000\n",
    "Steps_Till_Backprop = 64\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "epsilon = 0.99\n",
    "decay = 0.9999\n",
    "min_epsilon = 0.1\n",
    "episodes = 20000 #Set this to longer\n",
    "precision = 5\n",
    "Discard_Q_Value = -1e7\n",
    "Metric_Titles = ['Max_Q','Avg_Q','Min_Q']\n",
    "N_Downsampling_Episodes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI gym environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = make('connectx', debug=False)\n",
    "\n",
    "        # Define required gym fields (examples):\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.env.step(actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def render(self, **kwargs):\n",
    "        return self.env.render(**kwargs)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.env.state\n",
    "\n",
    "    def game_over(self):\n",
    "        return self.env.done\n",
    "\n",
    "    def current_player(self):\n",
    "        active = -1\n",
    "        if self.env.state[0].status == \"ACTIVE\":\n",
    "            active=0\n",
    "        if self.env.state[1].status == \"ACTIVE\":\n",
    "            active=1\n",
    "        return active\n",
    "\n",
    "    def get_configuration(self):\n",
    "        return self.env.configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
    "        self.hidden_layers = []\n",
    "        for i in hidden_units:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(i, activation='relu', kernel_initializer='he_normal'))\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions, activation='tanh', kernel_initializer='RandomNormal')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the tanh output layer which restricts output to \\[-1,1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.keras.optimizers.Nadam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = DeepModel(num_states, hidden_units, num_actions)\n",
    "        self.experience = {'inputs': [], 'a': [], 'r': [], 'inputs2': [], 'done': []} # The buffer\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
    "\n",
    "    #@tf.function\n",
    "    def train(self, TargetNet):\n",
    "        # Only start the training process when we have enough experiences in the buffer\n",
    "        if len(self.experience['inputs']) < self.min_experiences:\n",
    "            return 0\n",
    "\n",
    "        # Randomly select n experience in the buffer, n is batch-size\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['inputs']), size=self.batch_size)\n",
    "        states = np.asarray([self.experience['inputs'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "\n",
    "        # Prepare labels for training process\n",
    "        states_next = np.asarray([self.experience['inputs2'][i] for i in ids])\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        \n",
    "        # Find the value of the next states by computing the max over valid actions in these next states\n",
    "        Move_Validity = states_next[:,:self.num_actions]==0\n",
    "        Next_Q_Values = TargetNet.predict(states_next)\n",
    "        Next_Q_Values = np.where(Move_Validity,Next_Q_Values,Discard_Q_Value)\n",
    "        value_next = -np.max(Next_Q_Values,axis=1)\n",
    "        \n",
    "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            selected_action_values = tf.math.reduce_sum(self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
    "            loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Get an action by using epsilon-greedy\n",
    "    def get_action(self, state, epsilon):\n",
    "        prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n",
    "        if np.random.random() < epsilon:\n",
    "            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0])), prediction\n",
    "        else:\n",
    "            for i in range(self.num_actions):\n",
    "                if state.board[i] != 0:\n",
    "                    prediction[i] = Discard_Q_Value\n",
    "            return int(np.argmax(prediction)) , prediction\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['inputs']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        ref_model = tf.keras.Sequential()\n",
    "\n",
    "        ref_model.add(self.model.input_layer)\n",
    "        for layer in self.model.hidden_layers:\n",
    "            ref_model.add(layer)\n",
    "        ref_model.add(self.model.output_layer)\n",
    "\n",
    "        ref_model.load_weights(path)\n",
    "    \n",
    "    # Each state is represented as 1s for the current player's pieces, -1s for the opponent's pieces and 0s\n",
    "    def preprocess(self, state):\n",
    "        return np.array([1 if val==state.mark else 0 if val==0 else -1 for val in state.board])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that in the DQN::preprocess() method we represent the state for the current player. The current player (given by state.mark==1/2) will see his pieces as 1s and the opponent's pieces as -1s. Once he plays, the next player will see the state \"from his point of view\" where all *his* pieces are 1s. That way the Neural network does not make a distinction between playing player 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will play 1 ConnectX game and learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter):\n",
    "    turns = 0\n",
    "    env.reset()\n",
    "    Metric_Buffer={key:[] for key in Metric_Titles}\n",
    "    while not env.game_over():\n",
    "        active = env.current_player()\n",
    "\n",
    "        # Using epsilon-greedy to get an action\n",
    "        observations = env.get_state()[active].observation\n",
    "        action, Q_Values = TrainNet.get_action(observations, epsilon)\n",
    "        Q_Values = [val for val in Q_Values if val!=Discard_Q_Value]\n",
    "        Metric_Buffer['Avg_Q'].append(np.mean(Q_Values))\n",
    "        Metric_Buffer['Max_Q'].append(np.max(Q_Values))\n",
    "        Metric_Buffer['Min_Q'].append(np.min(Q_Values))\n",
    "\n",
    "        # Caching the information of current state\n",
    "        prev_observations = observations\n",
    "\n",
    "        # Take action\n",
    "        env.step([action if i==active else None for i in [0,1]])\n",
    "\n",
    "        reward=env.get_state()[active].reward\n",
    "\n",
    "        #Convert environment's [0,0.5,1] reward scheme to [-1,1]\n",
    "        if env.game_over():\n",
    "            if reward == 1: # Won\n",
    "                reward = 1\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -1\n",
    "            else: # Draw\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        next_active = 1 if active==0 else 0\n",
    "        # Adding experience into buffer\n",
    "        observations = env.get_state()[next_active].observation\n",
    "        exp = {'inputs': TrainNet.preprocess(prev_observations), 'a': action, 'r': reward, 'inputs2': TrainNet.preprocess(observations), 'done': env.game_over()}\n",
    "        TrainNet.add_experience(exp)\n",
    "\n",
    "        turns += 1\n",
    "        total_turns = Global_Step_Counter+turns\n",
    "        # Train the training model by using experiences in buffer and the target model\n",
    "        if total_turns%Steps_Till_Backprop==0:\n",
    "            TrainNet.train(TargetNet)\n",
    "        if total_turns%copy_step==0:\n",
    "            # Update the weights of the target model when reaching enough \"copy step\"\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    results={key:[] for key in Metric_Titles}\n",
    "    for metric_name in Metric_Titles:\n",
    "        results[metric_name]=np.mean(Metric_Buffer[metric_name])\n",
    "    return results, turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training loop. Note here that we track some metrics (min/max/mean Q values). Although it may be hard to tell from these metrics what constitutes a good agent, tracking them can help you ascertain convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n",
      "  0%|          | 63/20000 [00:05<21:45, 15.27it/s, Steps=1400, Updates=11200.0]\n",
      "  1%|          | 246/20000 [00:18<23:02, 14.28it/s, Steps=5400, Updates=43200.0]\n",
      "  2%|▏         | 344/20000 [00:25<27:08, 12.07it/s, Steps=7600, Updates=60800.0]\n",
      "  4%|▎         | 715/20000 [00:53<25:03, 12.83it/s, Steps=15400, Updates=1.23e+5]\n",
      "  5%|▍         | 979/20000 [01:14<26:05, 12.15it/s, Steps=21200, Updates=1.7e+5] \n",
      "  5%|▌         | 1007/20000 [01:16<21:53, 14.46it/s, Steps=21800, Updates=1.74e+5]\n",
      "  7%|▋         | 1461/20000 [01:51<30:20, 10.19it/s, Steps=31600, Updates=2.53e+5]\n",
      "  9%|▉         | 1888/20000 [02:25<21:38, 13.95it/s, Steps=40800, Updates=3.26e+5]\n",
      "  9%|▉         | 1898/20000 [02:26<24:00, 12.57it/s, Steps=41000, Updates=3.28e+5]\n",
      " 10%|▉         | 1999/20000 [02:33<22:21, 13.42it/s, Steps=43200, Updates=3.46e+5]\n",
      " 11%|█         | 2171/20000 [02:47<26:14, 11.32it/s, Steps=47000, Updates=3.76e+5]\n",
      " 12%|█▏        | 2403/20000 [03:07<21:02, 13.94it/s, Steps=52000, Updates=4.16e+5]\n",
      " 13%|█▎        | 2645/20000 [03:25<25:30, 11.34it/s, Steps=57000, Updates=4.56e+5]\n",
      " 14%|█▎        | 2731/20000 [03:32<25:20, 11.36it/s, Steps=58800, Updates=4.7e+5] \n",
      " 15%|█▌        | 3030/20000 [03:56<22:02, 12.83it/s, Steps=65200, Updates=5.22e+5]\n",
      " 15%|█▌        | 3038/20000 [03:57<22:21, 12.64it/s, Steps=65400, Updates=5.23e+5]\n",
      " 16%|█▌        | 3156/20000 [04:06<18:34, 15.12it/s, Steps=67800, Updates=5.42e+5]\n",
      " 17%|█▋        | 3494/20000 [04:33<21:27, 12.82it/s, Steps=75200, Updates=6.02e+5]\n",
      " 18%|█▊        | 3577/20000 [04:39<25:14, 10.84it/s, Steps=77000, Updates=6.16e+5]\n",
      " 18%|█▊        | 3642/20000 [04:44<22:53, 11.91it/s, Steps=78400, Updates=6.27e+5]\n",
      " 19%|█▉        | 3820/20000 [04:59<18:40, 14.44it/s, Steps=82000, Updates=6.56e+5]\n",
      " 20%|█▉        | 3944/20000 [05:08<19:35, 13.66it/s, Steps=84600, Updates=6.77e+5]\n",
      " 20%|█▉        | 3954/20000 [05:09<21:01, 12.72it/s, Steps=84800, Updates=6.78e+5]\n",
      " 20%|█▉        | 3993/20000 [05:12<19:43, 13.53it/s, Steps=85600, Updates=6.85e+5]\n",
      " 20%|██        | 4064/20000 [05:17<18:56, 14.03it/s, Steps=87000, Updates=6.96e+5]\n",
      " 21%|██        | 4239/20000 [05:31<20:09, 13.03it/s, Steps=90800, Updates=7.26e+5]\n",
      " 22%|██▏       | 4326/20000 [05:37<20:00, 13.05it/s, Steps=92600, Updates=7.41e+5]\n",
      " 22%|██▏       | 4456/20000 [05:47<23:14, 11.15it/s, Steps=95400, Updates=7.63e+5]\n",
      " 23%|██▎       | 4607/20000 [05:59<22:48, 11.24it/s, Steps=98600, Updates=7.89e+5]\n",
      " 25%|██▍       | 4995/20000 [06:28<19:50, 12.60it/s, Steps=105600, Updates=8.45e+5]\n",
      " 31%|███       | 6215/20000 [07:48<14:55, 15.39it/s, Steps=124000, Updates=9.92e+5]\n",
      " 32%|███▏      | 6491/20000 [08:05<12:06, 18.59it/s, Steps=127800, Updates=1.02e+6]\n",
      " 33%|███▎      | 6533/20000 [08:07<13:48, 16.25it/s, Steps=128400, Updates=1.03e+6]\n",
      " 33%|███▎      | 6694/20000 [08:17<13:20, 16.62it/s, Steps=130600, Updates=1.04e+6]\n",
      " 34%|███▍      | 6791/20000 [08:23<13:28, 16.33it/s, Steps=132000, Updates=1.06e+6]\n",
      " 34%|███▍      | 6802/20000 [08:24<16:59, 12.95it/s, Steps=132200, Updates=1.06e+6]\n",
      " 35%|███▍      | 6907/20000 [08:31<11:16, 19.34it/s, Steps=133800, Updates=1.07e+6]\n",
      " 36%|███▌      | 7150/20000 [08:47<12:59, 16.49it/s, Steps=137400, Updates=1.1e+6] \n",
      " 38%|███▊      | 7524/20000 [09:11<12:03, 17.24it/s, Steps=143000, Updates=1.14e+6]\n",
      " 39%|███▉      | 7767/20000 [09:26<13:10, 15.48it/s, Steps=146600, Updates=1.17e+6]\n",
      " 41%|████▏     | 8295/20000 [10:03<16:14, 12.01it/s, Steps=155000, Updates=1.24e+6]\n",
      " 43%|████▎     | 8531/20000 [10:19<11:50, 16.13it/s, Steps=158800, Updates=1.27e+6]\n",
      " 44%|████▍     | 8840/20000 [10:40<12:24, 14.98it/s, Steps=163800, Updates=1.31e+6]\n",
      " 45%|████▍     | 8937/20000 [10:46<11:02, 16.71it/s, Steps=165200, Updates=1.32e+6]\n",
      " 45%|████▍     | 8950/20000 [10:47<12:42, 14.50it/s, Steps=165400, Updates=1.32e+6]\n",
      " 48%|████▊     | 9689/20000 [11:39<10:41, 16.07it/s, Steps=178000, Updates=1.42e+6]\n",
      " 49%|████▉     | 9820/20000 [11:49<13:13, 12.82it/s, Steps=180400, Updates=1.44e+6]\n",
      " 51%|█████     | 10248/20000 [12:21<14:23, 11.29it/s, Steps=188400, Updates=1.51e+6]\n",
      " 52%|█████▏    | 10466/20000 [12:38<10:55, 14.54it/s, Steps=192600, Updates=1.54e+6]\n",
      " 55%|█████▍    | 10924/20000 [13:13<13:20, 11.34it/s, Steps=201000, Updates=1.61e+6]\n",
      " 55%|█████▍    | 10937/20000 [13:14<10:14, 14.75it/s, Steps=201200, Updates=1.61e+6]\n",
      " 55%|█████▍    | 10972/20000 [13:16<10:45, 13.98it/s, Steps=201800, Updates=1.61e+6]\n",
      " 55%|█████▌    | 11042/20000 [13:22<12:22, 12.06it/s, Steps=203200, Updates=1.63e+6]\n",
      " 56%|█████▋    | 11264/20000 [13:38<12:13, 11.91it/s, Steps=207400, Updates=1.66e+6]\n",
      " 57%|█████▋    | 11337/20000 [13:44<11:48, 12.22it/s, Steps=208800, Updates=1.67e+6]\n",
      " 59%|█████▉    | 11815/20000 [14:23<12:02, 11.34it/s, Steps=218000, Updates=1.74e+6]\n",
      " 59%|█████▉    | 11838/20000 [14:25<09:53, 13.74it/s, Steps=218400, Updates=1.75e+6]\n",
      " 61%|██████    | 12102/20000 [14:46<10:48, 12.18it/s, Steps=223600, Updates=1.79e+6]\n",
      " 61%|██████    | 12151/20000 [14:50<11:37, 11.26it/s, Steps=224600, Updates=1.8e+6] \n",
      " 61%|██████▏   | 12286/20000 [15:01<10:12, 12.59it/s, Steps=227200, Updates=1.82e+6]\n",
      " 62%|██████▏   | 12404/20000 [15:10<09:42, 13.04it/s, Steps=229400, Updates=1.84e+6]\n",
      " 62%|██████▏   | 12426/20000 [15:12<09:27, 13.35it/s, Steps=229800, Updates=1.84e+6]\n",
      " 65%|██████▍   | 12955/20000 [15:52<10:07, 11.60it/s, Steps=239800, Updates=1.92e+6]\n",
      " 65%|██████▌   | 13009/20000 [15:56<08:49, 13.20it/s, Steps=240800, Updates=1.93e+6]\n",
      " 67%|██████▋   | 13305/20000 [16:18<07:36, 14.65it/s, Steps=246200, Updates=1.97e+6]\n",
      " 69%|██████▊   | 13733/20000 [16:50<08:01, 13.01it/s, Steps=254000, Updates=2.03e+6]\n",
      " 70%|███████   | 14009/20000 [17:10<07:00, 14.23it/s, Steps=259000, Updates=2.07e+6]\n",
      " 70%|███████   | 14066/20000 [17:15<06:24, 15.42it/s, Steps=260000, Updates=2.08e+6]\n",
      " 71%|███████   | 14158/20000 [17:21<07:03, 13.80it/s, Steps=261600, Updates=2.09e+6]\n",
      " 71%|███████   | 14173/20000 [17:22<05:29, 17.68it/s, Steps=261800, Updates=2.09e+6]\n",
      " 71%|███████▏  | 14286/20000 [17:30<08:00, 11.88it/s, Steps=263800, Updates=2.11e+6]\n",
      " 72%|███████▏  | 14328/20000 [17:34<07:00, 13.49it/s, Steps=264600, Updates=2.12e+6]\n",
      " 72%|███████▏  | 14363/20000 [17:36<06:28, 14.51it/s, Steps=265200, Updates=2.12e+6]\n",
      " 72%|███████▏  | 14425/20000 [17:41<07:40, 12.10it/s, Steps=266400, Updates=2.13e+6]\n",
      " 75%|███████▍  | 14993/20000 [18:22<06:38, 12.57it/s, Steps=276400, Updates=2.21e+6]\n",
      " 75%|███████▌  | 15095/20000 [18:28<04:42, 17.39it/s, Steps=277800, Updates=2.22e+6]\n",
      " 76%|███████▌  | 15241/20000 [18:37<05:06, 15.54it/s, Steps=280200, Updates=2.24e+6]\n",
      " 77%|███████▋  | 15382/20000 [18:47<06:15, 12.31it/s, Steps=282400, Updates=2.26e+6]\n",
      " 83%|████████▎ | 16664/20000 [20:09<03:49, 14.52it/s, Steps=302200, Updates=2.42e+6]\n",
      " 83%|████████▎ | 16690/20000 [20:10<03:32, 15.56it/s, Steps=302600, Updates=2.42e+6]\n",
      " 84%|████████▍ | 16823/20000 [20:20<04:22, 12.09it/s, Steps=304800, Updates=2.44e+6]\n",
      " 85%|████████▍ | 16970/20000 [20:31<04:20, 11.62it/s, Steps=307600, Updates=2.46e+6]\n",
      " 86%|████████▌ | 17206/20000 [20:49<03:25, 13.56it/s, Steps=312000, Updates=2.5e+6] \n",
      " 87%|████████▋ | 17399/20000 [21:02<02:51, 15.14it/s, Steps=315400, Updates=2.52e+6]\n",
      " 88%|████████▊ | 17545/20000 [21:12<02:54, 14.10it/s, Steps=317800, Updates=2.54e+6]\n",
      " 90%|█████████ | 18039/20000 [21:50<02:30, 13.02it/s, Steps=327600, Updates=2.62e+6]\n",
      " 91%|█████████ | 18244/20000 [22:05<02:20, 12.48it/s, Steps=331400, Updates=2.65e+6]\n",
      " 91%|█████████▏| 18282/20000 [22:08<02:30, 11.42it/s, Steps=332200, Updates=2.66e+6]\n",
      " 94%|█████████▎| 18731/20000 [22:42<01:39, 12.71it/s, Steps=340600, Updates=2.72e+6]\n",
      " 95%|█████████▌| 19008/20000 [23:03<01:16, 12.91it/s, Steps=345800, Updates=2.77e+6]\n",
      " 96%|█████████▌| 19232/20000 [23:19<00:55, 13.89it/s, Steps=350000, Updates=2.8e+6] \n",
      " 98%|█████████▊| 19607/20000 [23:46<00:31, 12.62it/s, Steps=356800, Updates=2.85e+6]\n",
      " 99%|█████████▊| 19722/20000 [23:54<00:15, 18.46it/s, Steps=358800, Updates=2.87e+6]\n",
      " 99%|█████████▉| 19881/20000 [24:05<00:08, 13.38it/s, Steps=361600, Updates=2.89e+6]\n",
      "100%|█████████▉| 19979/20000 [24:12<00:01, 14.64it/s, Steps=363400, Updates=2.91e+6]\n",
      "100%|█████████▉| 19999/20000 [24:14<00:00, 12.74it/s, Steps=363400, Updates=2.91e+6]\n",
      "100%|██████████| 20000/20000 [24:14<00:00, 13.75it/s, Steps=363400, Updates=2.91e+6]\n"
     ]
    }
   ],
   "source": [
    "env = ConnectX()\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "Metrics = {key:[] for key in Metric_Titles} #Here we will store metrics for plotting after training\n",
    "Metrics_Buffer = {key:[] for key in Metric_Titles} #Downsampling buffer\n",
    "\n",
    "# Initialize models\n",
    "TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "\n",
    "if Train:\n",
    "    Global_Step_Counter=0\n",
    "    pbar = tqdm(range(episodes))\n",
    "    pbar2 = tqdm()\n",
    "    for n in pbar:\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        results, steps = play_game(env, TrainNet, TargetNet, epsilon, copy_step, Global_Step_Counter)\n",
    "        Global_Step_Counter += steps\n",
    "        for metric_name in Metric_Titles:\n",
    "            Metrics_Buffer[metric_name].append(results[metric_name])\n",
    "\n",
    "        if Global_Step_Counter%N_Downsampling_Episodes==0:\n",
    "            for metric_name in Metric_Titles: #Downsample our metrics from the buffer\n",
    "                Metrics[metric_name].append(np.mean(Metrics_Buffer[metric_name]))\n",
    "                Metrics_Buffer[metric_name].clear()\n",
    "            pbar.set_postfix({\n",
    "                'Steps': Global_Step_Counter,\n",
    "                'Updates': Global_Step_Counter*batch_size/Steps_Till_Backprop\n",
    "            })\n",
    "\n",
    "            pbar2.set_postfix({\n",
    "                'max_Q': Metrics['Max_Q'][-1],\n",
    "                'avg_Q': Metrics['Avg_Q'][-1],\n",
    "                'min_Q': Metrics['Min_Q'][-1],\n",
    "                'epsilon': epsilon,\n",
    "                'turns': steps\n",
    "            })\n",
    "\n",
    "    def Plot(data,title):\n",
    "        plt.figure()\n",
    "        plt.plot(data)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(title)\n",
    "        plt.savefig(title+'.png')\n",
    "        plt.close()\n",
    "\n",
    "    for metric_name in Metric_Titles:\n",
    "        Plot(Metrics[metric_name],metric_name)\n",
    "\n",
    "    TrainNet.save_weights('./weights.h5')\n",
    "else:\n",
    "    TrainNet.load_weights('./weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our agent into a submission.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = []\n",
    "\n",
    "# Get all hidden layers' weights\n",
    "for i in range(len(hidden_units)):\n",
    "    fc_layers.extend([\n",
    "        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n",
    "        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n",
    "    ])\n",
    "\n",
    "# Get output layer's weights\n",
    "fc_layers.extend([\n",
    "    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n",
    "    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n",
    "])\n",
    "\n",
    "# Convert all layers into usable form before integrating to final agent\n",
    "fc_layers = list(map(\n",
    "    lambda x: str(list(np.round(x, precision))) \\\n",
    "        .replace('array(', '').replace(')', '') \\\n",
    "        .replace(' ', '') \\\n",
    "        .replace('\\n', ''),\n",
    "    fc_layers\n",
    "))\n",
    "fc_layers = np.reshape(fc_layers, (-1, 2))\n",
    "\n",
    "# Create the agent\n",
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "\n",
    "'''\n",
    "\n",
    "# Write hidden layers\n",
    "for i, (w, b) in enumerate(fc_layers[:-1]):\n",
    "    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n",
    "    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n",
    "# Write output layer\n",
    "my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n",
    "my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n",
    "\n",
    "my_agent += '''\n",
    "    board = observation.board[:]\n",
    "    out = np.array([1 if val==observation.mark else 0 if val==0 else -1 for val in board],np.float32)\n",
    "'''\n",
    "\n",
    "# Calculate hidden layers\n",
    "for i in range(len(fc_layers[:-1])):\n",
    "    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n",
    "    my_agent += '    out = np.maximum(0,out)\\n' # Relu function\n",
    "# Calculate output layer\n",
    "my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n",
    "my_agent += '    out = np.tanh(out)\\n'\n",
    "\n",
    "my_agent += '''\n",
    "    for i in range(configuration.columns):\n",
    "        if observation.board[i] != 0:\n",
    "            out[i] = -1e7\n",
    "\n",
    "    return int(np.argmax(out))\n",
    "    '''\n",
    "\n",
    "with open('submission.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against the builtin Negamax opponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Agent vs. Negamax Agent: 0.05\n",
      "Negamax Agent vs. My Agent: 0.95\n"
     ]
    }
   ],
   "source": [
    "from submission import my_agent\n",
    "\n",
    "def epsilon_greedify(agent,epsilon=0.05): #Greedify our agent so we don't play the same games over and over in strength evaluation\n",
    "    def greedified_agent(observation,configuration):\n",
    "        import random\n",
    "        if random.random()<epsilon:\n",
    "            return random.choice([i for i in range(num_actions) if observation.board[i]==0])\n",
    "        else:\n",
    "            return agent(observation,configuration)\n",
    "    return greedified_agent\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "my_agent = epsilon_greedify(my_agent)\n",
    "print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=20)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=20)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
